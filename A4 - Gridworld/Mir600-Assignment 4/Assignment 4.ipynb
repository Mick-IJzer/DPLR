{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import scipy.optimize\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRAMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coordinate():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'({self.x}, {self.y})'\n",
    "        \n",
    "        \n",
    "    def move(self, direction, probs):\n",
    "        p = np.random.uniform()\n",
    "        \n",
    "        if direction == 3 and self.y < NROWS-1 and p > probs[3]:\n",
    "            self.y += 1\n",
    "        elif direction == 2 and self.y > 0 and p > probs[2]:\n",
    "            self.y -= 1\n",
    "        elif direction == 1 and self.x < NCOLUMNS-1 and p > probs[1]:\n",
    "            self.x += 1\n",
    "        elif direction == 0 and self.x > 0 and p > probs[0]:\n",
    "            self.x -= 1\n",
    "            \n",
    "    def equals(self, coordinate):\n",
    "        return (self.x == coordinate.x and self.y == coordinate.y)\n",
    "    \n",
    "    def copy(self):\n",
    "        return Coordinate(self.x, self.y)\n",
    "    \n",
    "    def get_distance(self, coordinate):\n",
    "        return np.sqrt((self.x - coordinate.x)**2 + (self.y - coordinate.y)**2)\n",
    "        \n",
    "    \n",
    "class Gridworld():\n",
    "    def __init__(self, goal=Coordinate(0, 9)):\n",
    "        self.rows = NROWS\n",
    "        self.columns = NCOLUMNS\n",
    "        self.goal = goal\n",
    "        self.transitions = np.around(np.random.uniform(size=(NROWS, NCOLUMNS, 4)), 1)\n",
    "        self.qtable = np.zeros((NROWS, NCOLUMNS, 4))\n",
    "        \n",
    "        \n",
    "    def reset_qtable(self):\n",
    "        self.qtable = np.zeros((NROWS, NCOLUMNS, 4))\n",
    "\n",
    "            \n",
    "    def use_heuristic(self, start):\n",
    "        current_state = start.copy()\n",
    "        steps = 0\n",
    "        \n",
    "        while (not current_state.equals(self.goal)) and (steps <= 500):\n",
    "            move = self.get_heuristic_move(current_state)\n",
    "            current_state.move(move, self.transitions[current_state.y, current_state.x, :])\n",
    "            steps += 1\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    \n",
    "    def get_heuristic_move(self, current_state):\n",
    "        if current_state.x > self.goal.x:  # West\n",
    "            return 0\n",
    "        elif current_state.x < self.goal.x:  # East\n",
    "            return 1\n",
    "        \n",
    "        if current_state.y > self.goal.y:  # North\n",
    "            return 2\n",
    "        elif current_state.y < self.goal.y:  # South\n",
    "            return 3\n",
    "        \n",
    "        return np.random.choice(options)\n",
    "        \n",
    "        \n",
    "    def q_learning(self, start, gamma=0.99, epsilon=0.05, alpha=0.1, max_steps=100, method='regular'):\n",
    "        current_state = start.copy()\n",
    "        steps = 0\n",
    "        while (not current_state.equals(self.goal)) and (steps < max_steps):\n",
    "\n",
    "            move = np.random.choice(np.argwhere(self.qtable[current_state.y, current_state.x, :] == \n",
    "                                                self.qtable[current_state.y, current_state.x, :].max()).flatten())\n",
    "\n",
    "            if np.random.uniform() < epsilon:\n",
    "                move = np.random.choice([0, 1, 2, 3])\n",
    "                \n",
    "            next_state = current_state.copy()\n",
    "            next_state.move(move, self.transitions[next_state.y, next_state.x, :])\n",
    "\n",
    "            reward = self.get_reward(next_state, current_state, method)\n",
    "\n",
    "            self.qtable[current_state.y, current_state.x, move] = round(self.qtable[current_state.y, current_state.x, move] + \\\n",
    "                                               alpha*(reward + gamma*self.qtable[next_state.y, next_state.x, :].max() \\\n",
    "                                               - self.qtable[current_state.y, current_state.x, move]), 4)\n",
    "                \n",
    "            steps += 1\n",
    "            current_state = next_state.copy()\n",
    "        \n",
    "        return self.qtable        \n",
    "        \n",
    "    \n",
    "    def get_reward(self, current_state, previous_state, method):\n",
    "        if method == 'regular':\n",
    "            return 1 if current_state.equals(self.goal) else -1\n",
    "        elif method == 'modified':\n",
    "            if current_state.equals(self.goal):\n",
    "                return 1\n",
    "            else:\n",
    "                return -current_state.get_distance(self.goal)/max_d\n",
    "    \n",
    "    \n",
    "    def value_iteration(self):\n",
    "        values = np.zeros((NROWS, NCOLUMNS))\n",
    "        rewards = np.ones((NROWS, NCOLUMNS))*-1\n",
    "        rewards[self.goal.y, self.goal.x] = 1\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            new_values = np.zeros((NROWS, NCOLUMNS, 4))\n",
    "        \n",
    "            new_values[:, :, 0] = (((1-self.transitions[:, :, 0]) * (np.c_[np.ones(NROWS)*-1, rewards][:, :-1] + \n",
    "                                                    np.c_[np.ones(NROWS)*-1, values][:, :-1])) +\n",
    "                                  (self.transitions[:, :, 0] * (rewards + values)))\n",
    "\n",
    "            new_values[:, :, 1] = (((1-self.transitions[:, :, 1]) * (np.c_[rewards, np.ones(NROWS)*-1][:, 1:] + \n",
    "                                                    np.c_[values, np.ones(NROWS)*-1][:, 1:])) +\n",
    "                                  (self.transitions[:, :, 1] * (rewards + values)))\n",
    "\n",
    "            new_values[:, :, 2] = (((1-self.transitions[:, :, 2]) * (np.r_['0,2', np.ones(NCOLUMNS)*-1, rewards][:-1, :] + \n",
    "                                                    np.r_['0,2', np.ones(NCOLUMNS)*-1, values][:-1, :])) +\n",
    "                                  (self.transitions[:, :, 2] * (rewards + values)))\n",
    "\n",
    "            new_values[:, :, 3] = (((1-self.transitions[:, :, 3]) * (np.r_['0,2', rewards, np.ones(NCOLUMNS)*-1][1:, :] + \n",
    "                                                    np.r_['0,2', values, np.ones(NCOLUMNS)*-1][1:, :])) +\n",
    "                                  (self.transitions[:, :, 3] * (rewards + values)))\n",
    "        \n",
    "            \n",
    "            max_new_values = new_values.max(axis=2)\n",
    "            max_new_values -= max_new_values.min()\n",
    "            \n",
    "            if np.abs(values - max_new_values).max() < 1e-8:\n",
    "                break\n",
    "            else:\n",
    "                values = copy.deepcopy(max_new_values)\n",
    "                steps += 1\n",
    "        \n",
    "        print(f'Value iteration converged after {steps} steps')        \n",
    "        return values - values.max() + 1, new_values.argmax(axis=2)\n",
    "    \n",
    "    \n",
    "    def eval_policy(self, policy, start):\n",
    "        steps = 0\n",
    "        current_state = start.copy()\n",
    "        while (not current_state.equals(self.goal)) and (steps <= 500):\n",
    "            move = policy[current_state.y, current_state.x]            \n",
    "            current_state.move(move, self.transitions[current_state.y, current_state.x, :])\n",
    "            steps += 1\n",
    "        return steps\n",
    "        \n",
    "    \n",
    "    def plot_policy(self, values, policy, name=None, complete=True):\n",
    "        values = (values - values.min()) / (values.max() - values.min())\n",
    "        \n",
    "        probs = np.zeros((policy.shape[0], policy.shape[1]))\n",
    "        for i in range(policy.shape[0]):\n",
    "            for j in range(policy.shape[1]):\n",
    "                probs[i, j] = 1 - self.transitions[i, j, policy[i, j]]\n",
    "                \n",
    "        X, Y = np.meshgrid(np.arange(NCOLUMNS)+0.5, np.arange(NROWS)+0.5)\n",
    "        U = np.where(policy == 1, 1, np.where(policy == 0, -1, 0))\n",
    "        V = np.where(policy == 2, -1, np.where(policy == 3, 1, 0))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10,10))        \n",
    "        sns.heatmap(values, ax=ax, cmap='gray')\n",
    "        ax.quiver(X, Y, U, V, probs, edgecolors='w', cmap='RdYlGn')\n",
    "        \n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel('X-Coordinate', fontsize=15)\n",
    "        ax.set_ylabel('Y-Coordinate', fontsize=15)\n",
    "        fig.show()\n",
    "        \n",
    "        if name is not None:\n",
    "            fig.savefig(name)\n",
    "            fig, ax = plt.subplots(figsize=(10,10))       \n",
    "            \n",
    "            halfrows = int(NROWS/2)\n",
    "            halfcolumns = int(NCOLUMNS/2)\n",
    "            policy_part = policy[:halfrows, :halfcolumns]\n",
    "            value_part = values[:halfrows, :halfcolumns]\n",
    "\n",
    "            probs = np.zeros((policy_part.shape[0], policy_part.shape[1]))\n",
    "            for i in range(policy_part.shape[0]):\n",
    "                for j in range(policy_part.shape[1]):\n",
    "                    probs[i, j] = 1 - self.transitions[i, j, policy_part[i, j]]\n",
    "                    \n",
    "            X, Y = np.meshgrid(np.arange(halfcolumns)+0.5, np.arange(halfrows)+0.5)\n",
    "            U = np.where(policy_part == 1, 1, np.where(policy_part == 0, -1, 0))\n",
    "            V = np.where(policy_part == 2, -1, np.where(policy_part == 3, 1, 0))\n",
    "\n",
    "\n",
    "            sns.heatmap(value_part, ax=ax, cmap='gray')\n",
    "            ax.quiver(X, Y, U, V, probs, edgecolors='w', cmap='RdYlGn')\n",
    "            \n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel('X-Coordinate', fontsize=15)\n",
    "            ax.set_ylabel('Y-Coordinate', fontsize=15)\n",
    "            \n",
    "            fig.savefig(name+'_part')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(26)\n",
    "\n",
    "NROWS = 50\n",
    "NCOLUMNS = 50\n",
    "max_d = np.sqrt(NROWS**2 + NCOLUMNS**2)\n",
    "its = 10\n",
    "\n",
    "env = Gridworld(Coordinate(0, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = np.array([\n",
    "    Coordinate(5, 10),\n",
    "    Coordinate(10, 15),\n",
    "    Coordinate(10, 5),\n",
    "    Coordinate(25, 10),\n",
    "    Coordinate(15, 20),\n",
    "    Coordinate(15, 0),\n",
    "    Coordinate(15, 10),\n",
    "    Coordinate(0, 20),\n",
    "    Coordinate(20, 25),\n",
    "    Coordinate(5, 30),\n",
    "    Coordinate(35, 5),\n",
    "    Coordinate(35, 25),\n",
    "    Coordinate(10, 40),\n",
    "    Coordinate(25, 35),\n",
    "    Coordinate(30, 40),\n",
    "    Coordinate(30, 15),\n",
    "    Coordinate(15, 35),\n",
    "    Coordinate(45, 15),\n",
    "    Coordinate(20, 45),\n",
    "    Coordinate(40, 30),\n",
    "    Coordinate(40, 40),\n",
    "    Coordinate(45, 5),\n",
    "    Coordinate(5, 45),\n",
    "    Coordinate(35, 45),\n",
    "    Coordinate(45, 45)\n",
    "], dtype=Coordinate)\n",
    "\n",
    "\n",
    "distances = np.zeros(len(trainset))\n",
    "\n",
    "for i, start in enumerate(trainset):  \n",
    "    distance = start.get_distance(env.goal)\n",
    "    distances[i] = distance\n",
    "\n",
    "trainset = trainset[np.argsort(distances)]\n",
    "distances = np.sort(distances)\n",
    "\n",
    "mean_steps_taken = np.zeros((4, len(trainset)))\n",
    "std_steps_taken = np.zeros((4, len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_grid = np.zeros((NROWS, NCOLUMNS))\n",
    "\n",
    "for c in trainset:\n",
    "    start_grid[c.y, c.x] = 1\n",
    "\n",
    "start_grid[env.goal.y, env.goal.x] = -1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "sns.heatmap(start_grid, ax=ax, cmap='gray')\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEURISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.zeros((its, len(trainset)))\n",
    "for i, start in enumerate(trainset):\n",
    "    for j in range(its):\n",
    "        step = env.use_heuristic(start)\n",
    "        steps[j, i] = step\n",
    "        print (\"\\r [{}/{}][{}/{}]\".format(i+1, len(trainset), j+1, its), end=\"\")\n",
    "\n",
    "mean_steps_taken[0, :] = steps.mean(axis=0)\n",
    "std_steps_taken[0, :] = steps.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALUE ITERATION (DYNAMIC PROGRAMMING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_DP, policy_DP = env.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.zeros((its, len(trainset)))\n",
    "for i, start in enumerate(trainset):\n",
    "    for j in range(its):\n",
    "        step = env.eval_policy(policy_DP, start)\n",
    "        steps[j, i] = step\n",
    "        print (\"\\r [{}/{}][{}/{}]\".format(i+1, len(trainset), j+1, its), end=\"\")\n",
    "\n",
    "mean_steps_taken[1, :] = steps.mean(axis=0)\n",
    "std_steps_taken[1, :] = steps.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_policy(values_DP, policy_DP, 'Value Iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYSTEM OF EQUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.ones((NROWS, NCOLUMNS))*-1\n",
    "rewards[env.goal.y, env.goal.x] = 1\n",
    "\n",
    "\n",
    "def f(values, policy = False):\n",
    "    values = values.reshape(NROWS, NCOLUMNS)\n",
    "    \n",
    "    new_values = np.zeros((NROWS, NCOLUMNS, 4))\n",
    "    new_values[:, :, 0] = (((1-env.transitions[:, :, 0]) * (np.c_[np.ones(NROWS)*-1, rewards][:, :-1] + \n",
    "                                                    np.c_[np.ones(NROWS)*-1, values][:, :-1])) +\n",
    "                                  (env.transitions[:, :, 0] * (rewards + values)))\n",
    "\n",
    "    new_values[:, :, 1] = (((1-env.transitions[:, :, 1]) * (np.c_[rewards, np.ones(NROWS)*-1][:, 1:] + \n",
    "                                                    np.c_[values, np.ones(NROWS)*-1][:, 1:])) +\n",
    "                                  (env.transitions[:, :, 1] * (rewards + values)))\n",
    "\n",
    "    new_values[:, :, 2] = (((1-env.transitions[:, :, 2]) * (np.r_['0,2', np.ones(NCOLUMNS)*-1, rewards][:-1, :] + \n",
    "                                                    np.r_['0,2', np.ones(NCOLUMNS)*-1, values][:-1, :])) +\n",
    "                                  (env.transitions[:, :, 2] * (rewards + values)))\n",
    "\n",
    "    new_values[:, :, 3] = (((1-env.transitions[:, :, 3]) * (np.r_['0,2', rewards, np.ones(NCOLUMNS)*-1][1:, :] + \n",
    "                                                    np.r_['0,2', values, np.ones(NCOLUMNS)*-1][1:, :])) +\n",
    "                                  (env.transitions[:, :, 3] * (rewards + values)))\n",
    "\n",
    "\n",
    "    max_new_values = new_values.max(axis=2)\n",
    "    max_new_values -= max_new_values.min()\n",
    "    \n",
    "    if policy:\n",
    "        return max_new_values, new_values.argmax(axis=2)\n",
    "    return -(values - max_new_values).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scipy.optimize.excitingmixing(f, np.ones((NROWS, NCOLUMNS)), iter=1500)\n",
    "values_EQ, policy_EQ = f(x, policy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_policy(values_EQ, policy_EQ, 'System of Equations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(policy_DP == policy_EQ).sum() /(50*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LEARNING SET-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_its = 1500\n",
    "n_steps = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LEARNING (REGULAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.reset_qtable()\n",
    "for i, start in enumerate(trainset):\n",
    "    for j in range(n_its):\n",
    "        env.q_learning(start, max_steps=n_steps)\n",
    "        print (\"\\r [{}/{}][{}/{}]\".format(i+1, len(trainset), j+1, n_its), end=\"\")\n",
    "\n",
    "values_Qreg, policy_Qreg = env.qtable.max(axis=2), env.qtable.argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.zeros((its, len(trainset)))\n",
    "for i, start in enumerate(trainset):\n",
    "    for j in range(its):\n",
    "        step = env.eval_policy(policy_Qreg, start)\n",
    "        steps[j, i] = step\n",
    "    print (\"\\r Iteration: {}\".format(i+1), end=\"\")\n",
    "\n",
    "mean_steps_taken[2, :] = steps.mean(axis=0)\n",
    "std_steps_taken[2, :] = steps.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_policy(values_Qreg, policy_Qreg, 'Q-learning (reg)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LEARNING (MODIFIED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.reset_qtable()\n",
    "for i, start in enumerate(trainset):\n",
    "    for j in range(n_its):\n",
    "        env.q_learning(start, max_steps=n_steps, method='modified')\n",
    "        print (\"\\r [{}/{}][{}/{}]\".format(i+1, len(trainset), j+1, n_its), end=\"\")\n",
    "\n",
    "values_Qmod, policy_Qmod = env.qtable.max(axis=2), env.qtable.argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.zeros((its, len(trainset)))\n",
    "for i, start in enumerate(trainset):\n",
    "    for j in range(its):\n",
    "        step = env.eval_policy(policy_Qmod, start)\n",
    "        steps[j, i] = step\n",
    "    print (\"\\r Iteration: {}\".format(i+1), end=\"\")\n",
    "\n",
    "mean_steps_taken[3, :] = steps.mean(axis=0)\n",
    "std_steps_taken[3, :] = steps.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_policy(values_Qmod, policy_Qmod, 'Q-learning (mod)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "\n",
    "ax.plot(distances, mean_steps_taken[0], 'o', c='red', label='Heuristic', ms=10)\n",
    "ax.plot(distances, mean_steps_taken[1], 'v', c='blue', label='Value Iteration', ms=10)\n",
    "ax.plot(distances, mean_steps_taken[2], 's', c='green', label='Q-learning (reg)', ms=7)\n",
    "ax.plot(distances, mean_steps_taken[3], 'p', c='magenta', label='Q-learning (mod)', ms=7)\n",
    "\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(loc = 'center right', prop={'size': 12})\n",
    "ax.set_xlabel('Distance from goal', fontsize=15)\n",
    "ax.set_ylabel('Average steps needed', fontsize=15)\n",
    "fig.savefig('comparison')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(policy_DP == policy_Qmod).sum() / (NROWS*NCOLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(policy_DP == policy_Qreg).sum() / (NROWS*NCOLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(policy_Qmod == policy_Qreg).sum() / (NROWS*NCOLUMNS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
